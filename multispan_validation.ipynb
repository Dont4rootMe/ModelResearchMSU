{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, get_cosine_schedule_with_warmup\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import numpy as np\n",
    "import json\n",
    "import io\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_WINDOW = 900\n",
    "\n",
    "\n",
    "def get_start_end_pos(start, end, tokens_gaps):\n",
    "    tokens_match = []\n",
    "    tokens_gaps = tokens_gaps\n",
    "\n",
    "    for i, token_gap in enumerate(tokens_gaps[:-1]):\n",
    "        if token_gap[1] >= start and token_gap[0] <= end:\n",
    "            tokens_match.append(i)\n",
    "\n",
    "    first = tokens_match[0]\n",
    "    last  = tokens_match[-1]\n",
    "\n",
    "    if len(tokens_gaps) <= INPUT_WINDOW:\n",
    "        return 0, len(tokens_gaps), first, last\n",
    "    \n",
    "    space_available = (INPUT_WINDOW - (last - first)) // 2\n",
    "    offset_left  = np.maximum(0, first - space_available)\n",
    "    offset_right = np.minimum(len(tokens_gaps), last + space_available)\n",
    "\n",
    "    return offset_left, offset_right, first - offset_left, last - offset_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CultCode(Dataset):\n",
    "    def __init__(self, tokenizer, train_on_single_tags=False):\n",
    "        cult_code = json.load(io.open('./datasets/related/cult_code.json'))\n",
    "        markups = cult_code['dataset']['markups']\n",
    "\n",
    "        self.texts = []\n",
    "        self.uno_span = []\n",
    "        self.multi_span = []\n",
    "        for i, mrkp in enumerate(markups):\n",
    "            input_ids, token_type_ids, attention_mask, offset_mapping = tokenizer(mrkp['text'], return_tensors=\"pt\", return_offsets_mapping=True).values()\n",
    "            \n",
    "            for rel in mrkp['relations']:\n",
    "                if train_on_single_tags and len(rel['tags']) > 1:\n",
    "                    continue\n",
    "                \n",
    "                if len(rel['spans']) == 1:\n",
    "                    begin = mrkp['spans'][rel['spans'][0]]['begin']\n",
    "                    end   = mrkp['spans'][rel['spans'][0]]['end']\n",
    "                    label = rel['tags'][0]\n",
    "                    offset_left, offset_right, left_begin, right_begin = get_start_end_pos(begin, end, offset_mapping)\n",
    "                    \n",
    "                    self.uno_span.append(\n",
    "                        (label, \n",
    "                         input_ids[offset_left, offset_right], \n",
    "                         token_type_ids[offset_left, offset_right], \n",
    "                         attention_mask[offset_left, offset_right],\n",
    "                         left_begin, right_begin\n",
    "                        )\n",
    "                    )\n",
    "                \n",
    "                elif len(rel['spans']) > 1:\n",
    "                    mult = []\n",
    "                    for span in rel['spans']:\n",
    "                        begin = mrkp['spans'][span]['begin']\n",
    "                        end   = mrkp['spans'][span]['end']\n",
    "                        mult.append([begin, end])\n",
    "                    self.multi_span.append((i, mult))\n",
    "    \n",
    "    def get_len_eval(self):\n",
    "        return len(self.multi_span)\n",
    "    \n",
    "    def get_item_eval(self, index):\n",
    "        i, span = self.multi_span[index]\n",
    "        return {\n",
    "            'text': self.texts[i],\n",
    "            'span': span\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.uno_span)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        i, span = self.uno_span[index]\n",
    "        return {\n",
    "            'text': self.texts[i],\n",
    "            'span': span\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeSamples(Dataset):\n",
    "    def __init__(self, train_on_single_tags=False, ):\n",
    "        cult_code = json.load(io.open('./datasets/related/cult_code.json'))\n",
    "        markups = cult_code['dataset']['markups']\n",
    "        \n",
    "        self.texts = []\n",
    "        for i, mrkp in enumerate(markups):\n",
    "            self.texts.append(mrkp['text'])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts) * 100\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        txt_ind = torch.randint(low=0, high=len(self.texts), size=(1,)).item()\n",
    "        in_text_len = torch.randint(low=0, high=len(self.texts[txt_ind]) - 60, size=(1,)).item()\n",
    "        delta = torch.randint(low=0, high=len(self.texts), size=(1,)).item()\n",
    "        \n",
    "        return {\n",
    "            'text': self.texts[txt_ind],\n",
    "            'span': [in_text_len, in_text_len+delta]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiSpanValidator(torch.nn.Module):\n",
    "    def __init__(self, out_dim=768, encoder='microsoft/deberta-v3-base',\n",
    "                layers_to_freeze=5, device='cpu'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.out_dim = out_dim\n",
    "        self.device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(encoder)\n",
    "        self.model = AutoModel.from_pretrained(encoder)\n",
    "        \n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(out_dim, out_dim // 3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(out_dim // 3, 2)\n",
    "        )\n",
    "        \n",
    "        for layer in self.model.encoder.layer[:layers_to_freeze]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        text = x['text']\n",
    "        span = torch.stack(x['span']).T\n",
    "\n",
    "        batch = self.tokenizer(text, return_tensors='pt', padding=True, )\n",
    "        tokens_spans = []\n",
    "    \n",
    "        for i, (start, end) in enumerate(span):\n",
    "            end = max(end, len(text[i])-2)\n",
    "            token_start = batch.char_to_token(batch_or_char_index=i, char_index=start)\n",
    "            token_end = batch.char_to_token(batch_or_char_index=i, char_index=end)\n",
    "            if token_start is None or token_end is None:\n",
    "                continue\n",
    "            \n",
    "            tokens_spans.append((i, token_start, token_end))\n",
    "\n",
    "        if not len(tokens_spans):\n",
    "            return None\n",
    "\n",
    "        tens = self.model(**batch).last_hidden_state\n",
    "        out = []\n",
    "        for i, start, end in tokens_spans:\n",
    "            out.append(torch.mean(tens[i, start:end+1], dim=0))\n",
    "        out = torch.stack(out)\n",
    "\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/artemon/.local/share/virtualenvs/models-research-YeHw3bbB/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/Users/artemon/.local/share/virtualenvs/models-research-YeHw3bbB/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = MultiSpanValidator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Until recently, the prevailing view assumed lorem ipsum was born as a nonsense text. “It’s not Latin, though it looks like it, and it actually says nothing,” Before & After magazine answered a curious reader, “Its ‘words’ loosely approximate the frequency with which letters occur in English, which is why at a glance it looks pretty real.”\n",
    "\n",
    "As Cicero would put it, “Um, not so fast.”\n",
    "\n",
    "The placeholder text, beginning with the line “Lorem ipsum dolor sit amet, consectetur adipiscing elit”, looks like Latin because in its youth, centuries ago, it was Latin.\n",
    "\n",
    "Richard McClintock, a Latin scholar from Hampden-Sydney College, is credited with discovering the source behind the ubiquitous filler text. In seeing a sample of lorem ipsum, his interest was piqued by consectetur—a genuine, albeit rare, Latin word. Consulting a Latin dictionary led McClintock to a passage from De Finibus Bonorum et Malorum (“On the Extremes of Good and Evil”), a first-century B.C. text from the Roman philosopher Cicero.\n",
    "\n",
    "In particular, the garbled words of lorem ipsum bear an unmistakable resemblance to sections 1.10.32–33 of Cicero’s work, with the most notable passage excerpted below:\n",
    "\n",
    "“Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem.”\n",
    "\n",
    "A 1914 English translation by Harris Rackham reads:\n",
    "\n",
    "“Nor is there anyone who loves or pursues or desires to obtain pain of itself, because it is pain, but occasionally circumstances occur in which toil and pain can procure him some great pleasure.”\n",
    "\n",
    "McClintock’s eye for detail certainly helped narrow the whereabouts of lorem ipsum’s origin, however, the “how and when” still remain something of a mystery, with competing theories and timelines.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "425"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 6550, 1104, 261, 262, 17138, 866, 6510, 24442, 358, 106652, 284, 1338, 283, 266, 13003, 1529, 260, 317, 1325, 276, 268, 298, 4996, 261, 651, 278, 1127, 334, 278, 261, 263, 278, 675, 652, 942, 261, 318, 2306, 429, 643, 3421, 4951, 266, 5348, 3684, 261, 317, 1325, 268, 534, 31826, 276, 18841, 14942, 262, 4436, 275, 319, 3527, 3080, 267, 1342, 261, 319, 269, 579, 288, 266, 9192, 278, 1127, 890, 609, 260, 318, 463, 50362, 338, 552, 278, 261, 317, 48179, 261, 298, 324, 1274, 260, 318, 279, 11736, 1529, 261, 1547, 275, 262, 683, 317, 17183, 368, 358, 106652, 72880, 2146, 266, 13595, 261, 4636, 64706, 473, 18324, 266, 27744, 59714, 510, 5173, 1632, 318, 261, 1127, 334, 4996, 401, 267, 359, 3020, 261, 6156, 824, 261, 278, 284, 4996, 260, 3155, 99660, 261, 266, 4996, 13146, 292, 67795, 271, 74640, 1676, 261, 269, 11653, 275, 10006, 262, 1271, 931, 262, 18994, 18594, 1529, 260, 344, 1769, 266, 2783, 265, 24442, 358, 106652, 261, 315, 981, 284, 57436, 293, 4636, 64706, 473, 18324, 644, 452, 4863, 261, 12524, 2942, 261, 4996, 1180, 260, 12054, 266, 4996, 13144, 1379, 99660, 264, 266, 6155, 292, 2060, 16343, 46000, 11988, 31796, 1770, 10531, 31796, 287, 3204, 7030, 262, 16939, 268, 265, 1798, 263, 13967, 318, 285, 261, 266, 362, 271, 8474, 736, 260, 711, 260, 1529, 292, 262, 4552, 19218, 50362, 260, 344, 1070, 261, 262, 95428, 1023, 265, 24442, 358, 106652, 3872, 299, 34886, 23439, 264, 4341, 376, 260, 894, 260, 3418, 958, 4629, 265, 50362, 276, 268, 374, 261, 275, 262, 370, 7315, 6155, 92094, 748, 294, 317, 1609, 63575, 10342, 4759, 14710, 268, 77343, 11148, 261, 14710, 72880, 4379, 106652, 14710, 452, 72880, 2146, 266, 13595, 261, 4636, 64706, 473, 18324, 261, 266, 27744, 1890, 8673, 38594, 1632, 261, 40327, 14710, 452, 745, 36221, 77343, 865, 11434, 9284, 667, 93182, 452, 267, 33424, 26544, 24149, 3677, 473, 1770, 333, 40607, 87074, 358, 266, 108244, 358, 110288, 16988, 8626, 1025, 39181, 358, 260, 318, 336, 16473, 1342, 5493, 293, 6452, 19864, 5720, 8036, 294, 317, 46849, 269, 343, 1012, 328, 3721, 289, 48855, 289, 7930, 264, 2801, 1427, 265, 1161, 261, 401, 278, 269, 1427, 261, 304, 5586, 3061, 3080, 267, 319, 51485, 263, 1427, 295, 27587, 417, 347, 426, 3757, 260, 318, 99660, 276, 268, 1572, 270, 2353, 1475, 1666, 4591, 262, 26691, 265, 24442, 358, 106652, 276, 268, 5392, 261, 901, 261, 262, 317, 5608, 263, 335, 318, 449, 1809, 491, 265, 266, 5633, 261, 275, 6484, 7715, 263, 28620, 260, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 5), (5, 14), (14, 15), (15, 19), (19, 30), (30, 35), (35, 43), (43, 48), (48, 49), (49, 55), (55, 59), (59, 64), (64, 67), (67, 69), (69, 78), (78, 83), (83, 84), (84, 86), (86, 88), (88, 89), (89, 90), (90, 94), (94, 100), (100, 101), (101, 108), (108, 111), (111, 117), (117, 122), (122, 125), (125, 126), (126, 130), (130, 133), (133, 142), (142, 147), (147, 155), (155, 156), (156, 157), (157, 164), (164, 166), (166, 172), (172, 181), (181, 190), (190, 192), (192, 200), (200, 207), (207, 208), (208, 210), (210, 212), (212, 213), (213, 215), (215, 220), (220, 221), (221, 229), (229, 241), (241, 245), (245, 255), (255, 260), (260, 266), (266, 274), (274, 280), (280, 283), (283, 291), (291, 292), (292, 298), (298, 301), (301, 305), (305, 308), (308, 310), (310, 317), (317, 320), (320, 326), (326, 333), (333, 338), (338, 339), (339, 340), (341, 344), (344, 351), (351, 357), (357, 361), (361, 364), (364, 365), (365, 367), (367, 369), (369, 370), (370, 374), (374, 377), (377, 382), (382, 383), (383, 384), (385, 389), (389, 401), (401, 406), (406, 407), (407, 417), (417, 422), (422, 426), (426, 431), (431, 433), (433, 435), (435, 437), (437, 438), (438, 444), (444, 450), (450, 454), (454, 456), (456, 459), (459, 460), (460, 464), (464, 468), (468, 469), (469, 472), (472, 474), (474, 477), (477, 480), (480, 483), (483, 486), (486, 488), (488, 489), (489, 490), (490, 496), (496, 501), (501, 507), (507, 515), (515, 518), (518, 522), (522, 528), (528, 529), (529, 539), (539, 543), (543, 544), (544, 547), (547, 551), (551, 557), (557, 558), (559, 567), (567, 578), (578, 579), (579, 581), (581, 587), (587, 595), (595, 600), (600, 608), (608, 609), (609, 615), (615, 623), (623, 624), (624, 627), (627, 636), (636, 641), (641, 653), (653, 657), (657, 664), (664, 671), (671, 675), (675, 686), (686, 693), (693, 698), (698, 699), (699, 702), (702, 709), (709, 711), (711, 718), (718, 721), (721, 726), (726, 727), (727, 733), (733, 734), (734, 738), (738, 747), (747, 751), (751, 758), (758, 761), (761, 765), (765, 769), (769, 770), (770, 773), (773, 774), (774, 775), (775, 783), (783, 784), (784, 791), (791, 796), (796, 797), (797, 803), (803, 808), (808, 809), (809, 820), (820, 822), (822, 828), (828, 839), (839, 843), (843, 854), (854, 857), (857, 859), (859, 867), (867, 872), (872, 875), (875, 879), (879, 883), (883, 887), (887, 891), (891, 894), (894, 898), (898, 902), (902, 904), (904, 905), (905, 907), (907, 911), (911, 919), (919, 920), (920, 923), (923, 928), (928, 932), (932, 937), (937, 938), (938, 939), (939, 940), (940, 942), (942, 948), (948, 949), (949, 956), (956, 958), (958, 959), (959, 960), (960, 961), (961, 966), (966, 971), (971, 975), (975, 981), (981, 993), (993, 1000), (1000, 1001), (1002, 1005), (1005, 1016), (1016, 1017), (1017, 1021), (1021, 1029), (1029, 1035), (1035, 1038), (1038, 1043), (1043, 1044), (1044, 1050), (1050, 1055), (1055, 1058), (1058, 1071), (1071, 1083), (1083, 1086), (1086, 1095), (1095, 1097), (1097, 1098), (1098, 1100), (1100, 1101), (1101, 1103), (1103, 1104), (1104, 1106), (1106, 1109), (1109, 1116), (1116, 1117), (1117, 1118), (1118, 1123), (1123, 1124), (1124, 1129), (1129, 1133), (1133, 1138), (1138, 1146), (1146, 1154), (1154, 1164), (1164, 1170), (1170, 1171), (1172, 1174), (1174, 1175), (1175, 1179), (1179, 1183), (1183, 1185), (1185, 1189), (1189, 1190), (1190, 1194), (1194, 1198), (1198, 1199), (1199, 1203), (1203, 1209), (1209, 1211), (1211, 1217), (1217, 1221), (1221, 1222), (1222, 1228), (1228, 1232), (1232, 1234), (1234, 1237), (1237, 1238), (1238, 1242), (1242, 1246), (1246, 1247), (1247, 1250), (1250, 1251), (1251, 1253), (1253, 1256), (1256, 1258), (1258, 1260), (1260, 1264), (1264, 1266), (1266, 1267), (1267, 1271), (1271, 1275), (1275, 1276), (1276, 1280), (1280, 1284), (1284, 1288), (1288, 1290), (1290, 1293), (1293, 1297), (1297, 1298), (1298, 1305), (1305, 1306), (1306, 1309), (1309, 1312), (1312, 1315), (1315, 1318), (1318, 1324), (1324, 1325), (1325, 1328), (1328, 1331), (1331, 1335), (1335, 1341), (1341, 1342), (1342, 1344), (1344, 1349), (1349, 1350), (1350, 1355), (1355, 1358), (1358, 1362), (1362, 1364), (1364, 1368), (1368, 1369), (1369, 1370), (1370, 1371), (1372, 1374), (1374, 1379), (1379, 1387), (1387, 1399), (1399, 1402), (1402, 1409), (1409, 1414), (1414, 1417), (1417, 1423), (1423, 1424), (1425, 1427), (1427, 1430), (1430, 1433), (1433, 1439), (1439, 1446), (1446, 1450), (1450, 1456), (1456, 1459), (1459, 1467), (1467, 1470), (1470, 1478), (1478, 1481), (1481, 1488), (1488, 1493), (1493, 1496), (1496, 1503), (1503, 1504), (1504, 1512), (1512, 1515), (1515, 1518), (1518, 1523), (1523, 1524), (1524, 1528), (1528, 1541), (1541, 1555), (1555, 1561), (1561, 1564), (1564, 1570), (1570, 1575), (1575, 1579), (1579, 1584), (1584, 1588), (1588, 1596), (1596, 1600), (1600, 1605), (1605, 1611), (1611, 1620), (1620, 1621), (1621, 1622), (1623, 1634), (1634, 1635), (1635, 1636), (1636, 1640), (1640, 1644), (1644, 1651), (1651, 1661), (1661, 1668), (1668, 1675), (1675, 1679), (1679, 1691), (1691, 1694), (1694, 1699), (1699, 1700), (1700, 1706), (1706, 1707), (1707, 1708), (1708, 1715), (1715, 1716), (1716, 1724), (1724, 1725), (1725, 1729), (1729, 1731), (1731, 1734), (1734, 1738), (1738, 1743), (1743, 1744), (1744, 1750), (1750, 1757), (1757, 1767), (1767, 1770), (1770, 1772), (1772, 1780), (1780, 1781), (1781, 1786), (1786, 1796), (1796, 1805), (1805, 1809), (1809, 1819), (1819, 1820), (0, 0)]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer(text, return_offsets_mapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    model,\n",
    "    dts,\n",
    "    step\n",
    "):\n",
    "    model.eval()\n",
    "    \n",
    "    success_rate = 0\n",
    "    \n",
    "    for i in range(dts.get_len_eval()):\n",
    "        text, spans = dts.get_item_eval(1).values()\n",
    "        batch = model.tokenizer(text, return_tensors='pt', padding=True)\n",
    "        tens = model.model(**batch).last_hidden_state\n",
    "        \n",
    "        out = []\n",
    "\n",
    "        for token_start, token_end in spans:\n",
    "            token_start = batch.char_to_token(batch_or_char_index=0, char_index=token_start)\n",
    "            token_end = batch.char_to_token(batch_or_char_index=0, char_index=token_end)\n",
    "            out.append(torch.mean(tens[0, token_start:token_end+1], dim=0))\n",
    "            \n",
    "        tns = torch.stack(out, dim=0).mean(dim=0)\n",
    "\n",
    "        success_rate += (model.classifier(tns.unsqueeze(0))[0, 1] > 0).item()\n",
    "        \n",
    "    print(f'test/success: {success_rate / dts.get_len_eval()}, step={step}')\n",
    "    \n",
    "def train(\n",
    "    model,\n",
    "    loss, \n",
    "    optim, \n",
    "    scheduler,\n",
    "    clt_dtl,\n",
    "    neg_dtl\n",
    "):\n",
    "    model.train()\n",
    "    \n",
    "    neg_iterator = iter(neg_dtl)\n",
    "    \n",
    "    for pos_batch in clt_dtl:\n",
    "        optim.zero_grad()\n",
    "        neg_batch = next(neg_iterator)\n",
    "        \n",
    "        try:\n",
    "            out_pos = model(pos_batch)\n",
    "            out_neg = model(neg_batch)\n",
    "            out = torch.concat([out_pos, out_neg], dim=0)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        target_ones = torch.ones(len(out_pos))\n",
    "        target_zeros = torch.zeros(len(out_neg))\n",
    "        targets = torch.concat([target_ones, target_zeros]).type(torch.long)\n",
    "\n",
    "        ls = loss(out, targets)\n",
    "        ls.backward()\n",
    "        \n",
    "        optim.step()\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "def train_loop(\n",
    "    model, \n",
    "    loss,\n",
    "    optim, \n",
    "    scheduler,\n",
    "    clt_dtl,\n",
    "    neg_dtl,\n",
    "    clt_dts,\n",
    "    num_training_steps\n",
    "):\n",
    "    for i in range(num_training_steps):\n",
    "        if i % 250 == 0:\n",
    "            # evaluate(model, clt_dts, i)\n",
    "            pass\n",
    "    \n",
    "        train(model, loss, optim, scheduler, clt_dtl, neg_dtl)\n",
    "        \n",
    "    torch.save(model.state_dict(), 'model.kpl')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'batch_size': 2,\n",
    "    'shuffle': True,\n",
    "    'pin_memory_device': True,\n",
    "    'device': 'cpu',\n",
    "    'num_warmup_steps': 250,\n",
    "    'num_training_steps': 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/artemon/.local/share/virtualenvs/models-research-YeHw3bbB/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/Users/artemon/.local/share/virtualenvs/models-research-YeHw3bbB/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "cltcode = CultCode()\n",
    "ngtsmpl = NegativeSamples()\n",
    "\n",
    "dtl_clt = DataLoader(cltcode, batch_size=config['batch_size'], shuffle=config['shuffle'], pin_memory_device=config['device'])\n",
    "dtl_neg = DataLoader(ngtsmpl, batch_size=config['batch_size'], shuffle=config['shuffle'], pin_memory_device=config['device'])\n",
    "\n",
    "model = MultiSpanValidator(device=config['device'])\n",
    "model.to(config['device'])\n",
    "loss = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "optim = torch.optim.AdamW(model.parameters())\n",
    "scheduler = get_cosine_schedule_with_warmup(optim, num_warmup_steps=config['num_warmup_steps'], num_training_steps=config['num_training_steps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/artemon/.local/share/virtualenvs/models-research-YeHw3bbB/lib/python3.11/site-packages/torch/utils/data/dataloader.py:594: UserWarning: pin memory device is set and pin_memory flag is not used then device pinned memory won't be usedplease set pin_memory to true, if you need to use the device pin memory\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, loss, optim, scheduler, dtl_clt, dtl_neg, cltcode, config['num_training_steps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cult_code = json.load(io.open('./datasets/related/cult_code.json'))\n",
    "markups = cult_code['dataset']['markups']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = defaultdict(lambda: 0)\n",
    "for mrkp in markups:\n",
    "    for rel in mrkp['relations']:\n",
    "        if len(rel['spans']) == 1 and len(rel['tags']) == 2:\n",
    "            for tag in rel['tags']:\n",
    "                labels[tag]+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl = {}\n",
    "for i in range(113):\n",
    "    lbl[cult_code['dataset']['relation_tags'][i]] = labels[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Материальные ценности'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = sorted(lbl, key=lambda k: lbl[k])\n",
    "keys[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl = {}\n",
    "for i in range(113):\n",
    "    lbl[cult_code['dataset']['relation_tags'][i]] = labels[i], i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 22)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl['Правосознание (гражданская активность, гражданственность)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'assessor': 4,\n",
       " 'text': '23 февраля в войсковой части 5526 прошло чествование воспитанников военно-патриотического клуба «Крепость». Заместитель командира части по идеологической работе майор Олег Ляшук отметил, что воспитание подрастающего поколения в патриотическом ключе, в стремлении к здоровому образу жизни, уважению к традициям, культурным ценностям и исторической памяти государства является главным профилактическим фактором безнравственности и аморальности. «Вместе мы вносим огромный вклад в будущее страны и нравственное здоровье нашего общества. Служба Родине во все времена была почетна. А служить можно по-разному, и не обязательно с оружием в руках. Служить можно в любом возрасте, служить можно и парню, и девушке. Служить можно и словом, и делом!» – подытожил Олег Ляшук.',\n",
       " 'spans': [{'begin': 191, 'end': 382, 'id': 0, 'tags': []},\n",
       "  {'begin': 444, 'end': 576, 'id': 1, 'tags': []}],\n",
       " 'relations': [{'spans': [0], 'tags': [1, 5, 10, 2, 0]},\n",
       "  {'spans': [1], 'tags': [0, 9, 1]}]}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markups[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_cents_1 = []\n",
    "for ind, mrkp in enumerate(markups):\n",
    "    for ind_2, rel in enumerate(mrkp['relations']):\n",
    "        if len(rel['spans']) > 1 and len(rel['tags']) == 2 and 22 in rel['tags']:\n",
    "            mat_cents_1.append((ind, ind_2, mrkp['text'], [(mrkp['spans'][sp]['begin'], mrkp['spans'][sp]['end']) for sp in rel['spans']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mat_cents_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_cents = []\n",
    "for ind, mrkp in enumerate(markups):\n",
    "    for ind_2, rel in enumerate(mrkp['relations']):\n",
    "        if len(rel['spans']) > 1 and len(rel['tags']) == 2 and 22 in rel['tags']:\n",
    "            mat_cents.append((ind, ind_2, mrkp['text'], [(mrkp['spans'][sp]['begin'], mrkp['spans'][sp]['end']) for sp in rel['spans']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mat_cents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_dts(dts):\n",
    "    for mrkp_index, rel_index, text, spans in dts:\n",
    "        print(f'({mrkp_index}, {rel_index})')\n",
    "        print(text)\n",
    "        print('=====')\n",
    "        for i, sp in enumerate(spans):\n",
    "            print(f'{i}: {text[sp[0]:sp[1]]}')\n",
    "        yield ''\n",
    "\n",
    "iter = iter_dts(mat_cents_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[261], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "next(iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mat_cents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "awkward = [\n",
    "    (45, 5),\n",
    "    (120, 0),\n",
    "    (404, 0)\n",
    "]\n",
    "\n",
    "semigrate = [\n",
    "    (94, 5),\n",
    "    (151, 3),\n",
    "    (156, 0),\n",
    "    (170, 0),\n",
    "    (172, 0),\n",
    "    (256, 1),\n",
    "    (281, 0),\n",
    "    (281, 4),\n",
    "    (362, 2),\n",
    "    (389, 2),\n",
    "    (450, 1),\n",
    "    (541, 0),\n",
    "    (562, 0),\n",
    "    (624, 9),\n",
    "    (665, 0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "printing = []\n",
    "for ind, mrkp in enumerate(markups):\n",
    "    for ind_2, rel in enumerate(mrkp['relations']):\n",
    "        if (ind, ind_2) in awkward:\n",
    "            printing.append((ind, ind_2, mrkp['text'], [(mrkp['spans'][sp]['begin'], mrkp['spans'][sp]['end']) for sp in rel['spans']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(printing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_dts(dts):\n",
    "    for mrkp_index, rel_index, text, spans in dts:\n",
    "        print(f'({mrkp_index}, {rel_index})')\n",
    "        print(text)\n",
    "        print('=====')\n",
    "        for i, sp in enumerate(spans):\n",
    "            print(f'{i}: {text[sp[0]:sp[1]]}')\n",
    "        yield ''\n",
    "\n",
    "iter = iter_dts(printing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[250], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "next(iter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "models-research-YeHw3bbB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
