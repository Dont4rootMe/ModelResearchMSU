{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/artemon/.local/share/virtualenvs/models-research-YeHw3bbB/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from gliner import GLiNER\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/artemon/.local/share/virtualenvs/models-research-YeHw3bbB/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "/Users/artemon/.local/share/virtualenvs/models-research-YeHw3bbB/lib/python3.11/site-packages/gliner/model.py:420: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_file, map_location=torch.device(map_location))\n"
     ]
    }
   ],
   "source": [
    "model = GLiNER.from_pretrained('numind/NuNerZero')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./dump/dummy_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hellow', 0, 6),\n",
       " ('fedorov', 7, 14),\n",
       " ('artem', 15, 20),\n",
       " (',', 20, 21),\n",
       " ('how', 22, 25),\n",
       " ('are', 26, 29),\n",
       " ('you', 30, 33),\n",
       " ('?', 33, 34)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Hellow fedorov artem, how are you?'\n",
    "labels = ['name', 'are']\n",
    "tokens = model.token_splitter(text)\n",
    "[*tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 7,\n",
       "  'end': 14,\n",
       "  'text': 'fedorov',\n",
       "  'label': 'name',\n",
       "  'score': 0.9380099773406982},\n",
       " {'start': 15,\n",
       "  'end': 20,\n",
       "  'text': 'artem',\n",
       "  'label': 'name',\n",
       "  'score': 0.9013848900794983}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_entities(text, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fedorov artem => name\n"
     ]
    }
   ],
   "source": [
    "def merge_entities(entities):\n",
    "    if not entities:\n",
    "        return []\n",
    "    merged = []\n",
    "    current = entities[0]\n",
    "    for next_entity in entities[1:]:\n",
    "        if next_entity['label'] == current['label'] and (next_entity['start'] == current['end'] + 1 or next_entity['start'] == current['end']):\n",
    "            current['text'] = text[current['start']: next_entity['end']].strip()\n",
    "            current['end'] = next_entity['end']\n",
    "        else:\n",
    "            merged.append(current)\n",
    "            current = next_entity\n",
    "    # Append the last entity\n",
    "    merged.append(current)\n",
    "    return merged\n",
    "\n",
    "\n",
    "# NuZero requires labels to be lower-cased!\n",
    "# labels = [\"organization\", \"initiative\", \"project\"]\n",
    "# labels = [l.lower() for l in labels]\n",
    "\n",
    "# text = \"At the annual technology summit, the keynote address was delivered by a senior member of the Association for Computing Machinery Special Interest Group on Algorithms and Computation Theory, which recently launched an expansive initiative titled 'Quantum Computing and Algorithmic Innovations: Shaping the Future of Technology'. This initiative explores the implications of quantum mechanics on next-generation computing and algorithm design and is part of a broader effort that includes the 'Global Computational Science Advancement Project'. The latter focuses on enhancing computational methodologies across scientific disciplines, aiming to set new benchmarks in computational efficiency and accuracy.\"\n",
    "\n",
    "entities = model.predict_entities(text, labels)\n",
    "\n",
    "entities = merge_entities(entities)\n",
    "\n",
    "for entity in entities:\n",
    "    print(entity[\"text\"], \"=>\", entity[\"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def batch_predict_entities(self, texts, labels, flat_ner=True, threshold=0.5, multi_label=False):\n",
      "        \"\"\"\n",
      "        Predict entities for a batch of texts.\n",
      "        texts:  List of texts | List[str]\n",
      "        labels: List of labels | List[str]\n",
      "        ...\n",
      "        \"\"\"\n",
      "\n",
      "        all_tokens = []\n",
      "        all_start_token_idx_to_text_idx = []\n",
      "        all_end_token_idx_to_text_idx = []\n",
      "\n",
      "        for text in texts:\n",
      "            tokens = []\n",
      "            start_token_idx_to_text_idx = []\n",
      "            end_token_idx_to_text_idx = []\n",
      "            for token, start, end in self.token_splitter(text):\n",
      "                tokens.append(token)\n",
      "                start_token_idx_to_text_idx.append(start)\n",
      "                end_token_idx_to_text_idx.append(end)\n",
      "            all_tokens.append(tokens)\n",
      "            all_start_token_idx_to_text_idx.append(start_token_idx_to_text_idx)\n",
      "            all_end_token_idx_to_text_idx.append(end_token_idx_to_text_idx)\n",
      "\n",
      "        input_x = [{\"tokenized_text\": tk, \"ner\": None} for tk in all_tokens]\n",
      "        x = self.collate_fn(input_x, labels)\n",
      "        outputs = self.predict(x, flat_ner=flat_ner, threshold=threshold, multi_label=multi_label)\n",
      "\n",
      "        all_entities = []\n",
      "        for i, output in enumerate(outputs):\n",
      "            start_token_idx_to_text_idx = all_start_token_idx_to_text_idx[i]\n",
      "            end_token_idx_to_text_idx = all_end_token_idx_to_text_idx[i]\n",
      "            entities = []\n",
      "            for start_token_idx, end_token_idx, ent_type,ent_score in output:\n",
      "                start_text_idx = start_token_idx_to_text_idx[start_token_idx]\n",
      "                end_text_idx = end_token_idx_to_text_idx[end_token_idx]\n",
      "                entities.append({\n",
      "                    \"start\": start_token_idx_to_text_idx[start_token_idx],\n",
      "                    \"end\": end_token_idx_to_text_idx[end_token_idx],\n",
      "                    \"text\": texts[i][start_text_idx:end_text_idx],\n",
      "                    \"label\": ent_type,\n",
      "                    \"score\": ent_score\n",
      "                })\n",
      "            all_entities.append(entities)\n",
      "\n",
      "        return all_entities\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(model.batch_predict_entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hellow', 0, 6),\n",
       " ('fedorov', 7, 14),\n",
       " ('artem', 15, 20),\n",
       " (',', 20, 21),\n",
       " ('how', 22, 25),\n",
       " ('are', 26, 29),\n",
       " ('you', 30, 33),\n",
       " ('?', 33, 34)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[*model.token_splitter(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = []\n",
    "all_start_token_idx_to_text_idx = []\n",
    "all_end_token_idx_to_text_idx = []\n",
    "\n",
    "for text in texts:\n",
    "    tokens = []\n",
    "    start_token_idx_to_text_idx = []\n",
    "    end_token_idx_to_text_idx = []\n",
    "    for token, start, end in model.token_splitter(text):\n",
    "        tokens.append(token)\n",
    "        start_token_idx_to_text_idx.append(start)\n",
    "        end_token_idx_to_text_idx.append(end)\n",
    "    all_tokens.append(tokens)\n",
    "    all_start_token_idx_to_text_idx.append(start_token_idx_to_text_idx)\n",
    "    all_end_token_idx_to_text_idx.append(end_token_idx_to_text_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_x = [{\"tokenized_text\": tk, \"ner\": None} for tk in all_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['Hellow', 'fedorov', 'artem', ',', 'how', 'are', 'you', '?']],\n",
       " [[0, 7, 15, 20, 22, 26, 30, 33]],\n",
       " [[6, 14, 20, 21, 25, 29, 33, 34]],\n",
       " [{'tokenized_text': ['Hellow',\n",
       "    'fedorov',\n",
       "    'artem',\n",
       "    ',',\n",
       "    'how',\n",
       "    'are',\n",
       "    'you',\n",
       "    '?'],\n",
       "   'ner': None}])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens, all_start_token_idx_to_text_idx, all_end_token_idx_to_text_idx, input_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['name', 'city location far away from home']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seq_length': tensor([8]),\n",
       " 'span_idx': tensor([[[0, 0],\n",
       "          [1, 1],\n",
       "          [2, 2],\n",
       "          [3, 3],\n",
       "          [4, 4],\n",
       "          [5, 5],\n",
       "          [6, 6],\n",
       "          [7, 7]]]),\n",
       " 'tokens': [['Hellow', 'fedorov', 'artem', ',', 'how', 'are', 'you', '?']],\n",
       " 'span_mask': tensor([[True, True, True, True, True, True, True, True]]),\n",
       " 'span_label': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'entities': [None],\n",
       " 'classes_to_id': {'name': 1, 'city location far away from home': 2},\n",
       " 'id_to_classes': {1: 'name', 2: 'city location far away from home'}}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = model.collate_fn(input_x, labels)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.predict(x, flat_ner=True, threshold=0.5, multi_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(1, 1, 'name', 0.8806719183921814), (2, 2, 'name', 0.8604023456573486)]]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    @torch.no_grad()\n",
      "    def predict(self, x, flat_ner=False, threshold=0.5, multi_label=False):\n",
      "        self.eval()\n",
      "        local_scores = self.compute_score_eval(x, device=next(self.parameters()).device)\n",
      "        probs = torch.sigmoid(local_scores)\n",
      "\n",
      "        spans = []\n",
      "        for i, _ in enumerate(x[\"tokens\"]):\n",
      "            probs_i = probs[i]\n",
      "            \n",
      "            wh_i = [i.tolist() for i in torch.where(probs_i > threshold)]\n",
      "            span_i = []\n",
      "            \n",
      "            for s, k, c in zip(*wh_i):\n",
      "                if s + k < len(x[\"tokens\"][i]):\n",
      "                    span_i.append((s, s + k, x[\"id_to_classes\"][c + 1], probs_i[s, k, c].item()))\n",
      "            \n",
      "            span_i = greedy_search(span_i, flat_ner, multi_label=multi_label)\n",
      "            spans.append(span_i)\n",
      "        return spans\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(model.predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seq_length': tensor([8]),\n",
       " 'span_idx': tensor([[[0, 0],\n",
       "          [1, 1],\n",
       "          [2, 2],\n",
       "          [3, 3],\n",
       "          [4, 4],\n",
       "          [5, 5],\n",
       "          [6, 6],\n",
       "          [7, 7]]]),\n",
       " 'tokens': [['Hellow', 'fedorov', 'artem', ',', 'how', 'are', 'you', '?']],\n",
       " 'span_mask': tensor([[True, True, True, True, True, True, True, True]]),\n",
       " 'span_label': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'entities': [None],\n",
       " 'classes_to_id': {'name': 1, 'city location far away from home': 2},\n",
       " 'id_to_classes': {1: 'name', 2: 'city location far away from home'}}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def compute_score_eval(self, x, device):\n",
      "        # check if classes_to_id is dict\n",
      "        assert isinstance(x[\"classes_to_id\"], dict), \"classes_to_id must be a dict\"\n",
      "\n",
      "        span_idx = (x[\"span_idx\"] * x[\"span_mask\"].unsqueeze(-1)).to(device)\n",
      "\n",
      "        all_types = list(x[\"classes_to_id\"].keys())\n",
      "        # multiple entity types in all_types. Prompt is appended at the start of tokens\n",
      "        entity_prompt = []\n",
      "\n",
      "        # add enity types to prompt\n",
      "        for entity_type in all_types:\n",
      "            entity_prompt.append(self.entity_token)\n",
      "            entity_prompt.append(entity_type)\n",
      "\n",
      "        entity_prompt.append(self.sep_token)\n",
      "\n",
      "        prompt_entity_length = len(entity_prompt)\n",
      "\n",
      "        # add prompt\n",
      "        tokens_p = [entity_prompt + tokens for tokens in x[\"tokens\"]]\n",
      "        seq_length_p = x[\"seq_length\"] + prompt_entity_length\n",
      "\n",
      "        out = self.token_rep_layer(tokens_p, seq_length_p)\n",
      "\n",
      "        word_rep_w_prompt = out[\"embeddings\"]\n",
      "        mask_w_prompt = out[\"mask\"]\n",
      "\n",
      "        # remove prompt\n",
      "        word_rep = word_rep_w_prompt[:, prompt_entity_length:, :]\n",
      "        mask = mask_w_prompt[:, prompt_entity_length:]\n",
      "\n",
      "        # get_entity_type_rep\n",
      "        entity_type_rep = word_rep_w_prompt[:, :prompt_entity_length - 1, :]\n",
      "        # extract [ENT] tokens (which are at even positions in entity_type_rep)\n",
      "        entity_type_rep = entity_type_rep[:, 0::2, :]\n",
      "\n",
      "        entity_type_rep = self.prompt_rep_layer(entity_type_rep)  # (batch_size, len_types, hidden_size)\n",
      "\n",
      "        word_rep = self.rnn(word_rep, mask)\n",
      "\n",
      "        span_rep = self.span_rep_layer(word_rep, span_idx)\n",
      "\n",
      "        local_scores = torch.einsum(\"BLKD,BCD->BLKC\", span_rep, entity_type_rep)\n",
      "\n",
      "        return local_scores\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(model.compute_score_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<<ENT>>', '<<SEP>>')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.entity_token, model.sep_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 9, 768]), torch.Size([1, 21]))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model.token_rep_layer([['<<ENT>>', 'name', 'hellow', '<<ENT>>', 'location', '<<SEP>>', 'Hellow', 'fedorov', 'artem']], torch.tensor([21]))\n",
    "\n",
    "out['embeddings'].shape, out['mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "module, class, method, function, traceback, frame, or code object was expected, got Sequential",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindsource\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_rep_layer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/inspect.py:1063\u001b[0m, in \u001b[0;36mfindsource\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfindsource\u001b[39m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the entire source file and starting line number for an object.\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \n\u001b[1;32m   1058\u001b[0m \u001b[38;5;124;03m    The argument may be a module, class, method, function, traceback, frame,\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;124;03m    or code object.  The source code is returned as a list of all the lines\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;124;03m    in the file and the line number indexes a line in that list.  An OSError\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;124;03m    is raised if the source code cannot be retrieved.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1063\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[43mgetsourcefile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file:\n\u001b[1;32m   1065\u001b[0m         \u001b[38;5;66;03m# Invalidate cache if needed.\u001b[39;00m\n\u001b[1;32m   1066\u001b[0m         linecache\u001b[38;5;241m.\u001b[39mcheckcache(file)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/inspect.py:940\u001b[0m, in \u001b[0;36mgetsourcefile\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetsourcefile\u001b[39m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[1;32m    937\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the filename that can be used to locate an object's source.\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;124;03m    Return None if no way can be identified to get the source.\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 940\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[43mgetfile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    941\u001b[0m     all_bytecode_suffixes \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mmachinery\u001b[38;5;241m.\u001b[39mDEBUG_BYTECODE_SUFFIXES[:]\n\u001b[1;32m    942\u001b[0m     all_bytecode_suffixes \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mmachinery\u001b[38;5;241m.\u001b[39mOPTIMIZED_BYTECODE_SUFFIXES[:]\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/models-research-YeHw3bbB/lib/python3.11/site-packages/torch/package/package_importer.py:698\u001b[0m, in \u001b[0;36m_patched_getfile\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m _package_imported_modules:\n\u001b[1;32m    697\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _package_imported_modules[\u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m]\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__file__\u001b[39m\n\u001b[0;32m--> 698\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_orig_getfile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/inspect.py:920\u001b[0m, in \u001b[0;36mgetfile\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m iscode(\u001b[38;5;28mobject\u001b[39m):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39mco_filename\n\u001b[0;32m--> 920\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodule, class, method, function, traceback, frame, or \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    921\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcode object was expected, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    922\u001b[0m                 \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: module, class, method, function, traceback, frame, or code object was expected, got Sequential"
     ]
    }
   ],
   "source": [
    "inspect.findsource(model.prompt_rep_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if classes_to_id is dict\n",
    "assert isinstance(x[\"classes_to_id\"], dict), \"classes_to_id must be a dict\"\n",
    "\n",
    "span_idx = (x[\"span_idx\"] * x[\"span_mask\"].unsqueeze(-1)).to('cpu')\n",
    "\n",
    "all_types = list(x[\"classes_to_id\"].keys())\n",
    "# multiple entity types in all_types. Prompt is appended at the start of tokens\n",
    "entity_prompt = []\n",
    "\n",
    "# add enity types to prompt\n",
    "for entity_type in all_types:\n",
    "    entity_prompt.append(model.entity_token)\n",
    "    entity_prompt.append(entity_type)\n",
    "\n",
    "entity_prompt.append(model.sep_token)\n",
    "\n",
    "prompt_entity_length = len(entity_prompt)\n",
    "\n",
    "# add prompt\n",
    "tokens_p = [entity_prompt + tokens for tokens in x[\"tokens\"]]\n",
    "seq_length_p = x[\"seq_length\"] + prompt_entity_length\n",
    "\n",
    "out = model.token_rep_layer(tokens_p, seq_length_p)\n",
    "\n",
    "word_rep_w_prompt = out[\"embeddings\"]\n",
    "mask_w_prompt = out[\"mask\"]\n",
    "\n",
    "word_rep = word_rep_w_prompt[:, prompt_entity_length:, :]\n",
    "mask = mask_w_prompt[:, prompt_entity_length:]\n",
    "\n",
    "# get_entity_type_rep\n",
    "entity_type_rep = word_rep_w_prompt[:, :prompt_entity_length - 1, :]\n",
    "# extract [ENT] tokens (which are at even positions in entity_type_rep)\n",
    "entity_type_rep = entity_type_rep[:, 0::2, :]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['<<ENT>>',\n",
       "   'name',\n",
       "   '<<ENT>>',\n",
       "   'city location far away from home',\n",
       "   '<<SEP>>',\n",
       "   'Hellow',\n",
       "   'fedorov',\n",
       "   'artem',\n",
       "   ',',\n",
       "   'how',\n",
       "   'are',\n",
       "   'you',\n",
       "   '?']],\n",
       " tensor([13]),\n",
       " torch.Size([1, 13, 768]))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_p, seq_length_p, word_rep_w_prompt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0586, -0.3570, -0.1945,  ..., -0.6190,  0.1706, -0.5615],\n",
       "         [ 0.2562, -0.4454,  0.4929,  ..., -0.0466,  0.5832, -0.2870]]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_type_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 768])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_rep_w_prompt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 768])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_rep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def forward(self, x):\n",
      "        # compute span representation\n",
      "        scores, num_classes, entity_type_mask = self.compute_score_train(x)\n",
      "        batch_size = scores.shape[0]\n",
      "\n",
      "        # loss for filtering classifier\n",
      "        logits_label = scores.view(-1, num_classes)\n",
      "        labels = x[\"span_label\"].view(-1)  # (batch_size * num_spans)\n",
      "        mask_label = labels != -1  # (batch_size * num_spans)\n",
      "        labels.masked_fill_(~mask_label, 0)  # Set the labels of padding tokens to 0\n",
      "\n",
      "        # one-hot encoding\n",
      "        labels_one_hot = torch.zeros(labels.size(0), num_classes + 1, dtype=torch.float32).to(scores.device)\n",
      "        labels_one_hot.scatter_(1, labels.unsqueeze(1), 1)  # Set the corresponding index to 1\n",
      "        labels_one_hot = labels_one_hot[:, 1:]  # Remove the first column\n",
      "        # Shape of labels_one_hot: (batch_size * num_spans, num_classes)\n",
      "\n",
      "        # compute loss (without reduction)\n",
      "        all_losses = F.binary_cross_entropy_with_logits(logits_label, labels_one_hot,\n",
      "                                                        reduction=\"none\")\n",
      "        # mask loss using entity_type_mask (B, C)\n",
      "        masked_loss = all_losses.view(batch_size, -1, num_classes) * entity_type_mask.unsqueeze(1)\n",
      "        all_losses = masked_loss.view(-1, num_classes)\n",
      "        # expand mask_label to all_losses\n",
      "        mask_label = mask_label.unsqueeze(-1).expand_as(all_losses)\n",
      "        # put lower loss for in label_one_hot (2 for positive, 1 for negative)\n",
      "        weight_c = labels_one_hot + 1\n",
      "        # apply mask\n",
      "        all_losses = all_losses * mask_label.float() * weight_c\n",
      "        return all_losses.sum()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(model.forward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def compute_score_train(self, x):\n",
      "        span_idx = x[\"span_idx\"] * x[\"span_mask\"].unsqueeze(-1)\n",
      "\n",
      "        new_length = x[\"seq_length\"].clone()\n",
      "        new_tokens = []\n",
      "        all_len_prompt = []\n",
      "        num_classes_all = []\n",
      "\n",
      "        # add prompt to the tokens\n",
      "        for i in range(len(x[\"tokens\"])):\n",
      "            all_types_i = list(x[\"classes_to_id\"][i].keys())\n",
      "            # multiple entity types in all_types. Prompt is appended at the start of tokens\n",
      "            entity_prompt = []\n",
      "            num_classes_all.append(len(all_types_i))\n",
      "            # add enity types to prompt\n",
      "            for entity_type in all_types_i:\n",
      "                entity_prompt.append(self.entity_token)  # [ENT] token\n",
      "                entity_prompt.append(entity_type)  # entity type\n",
      "            entity_prompt.append(self.sep_token)  # [SEP] token\n",
      "\n",
      "            # prompt format:\n",
      "            # [ENT] entity_type [ENT] entity_type ... [ENT] entity_type [SEP]\n",
      "\n",
      "            # add prompt to the tokens\n",
      "            tokens_p = entity_prompt + x[\"tokens\"][i]\n",
      "\n",
      "            # input format:\n",
      "            # [ENT] entity_type_1 [ENT] entity_type_2 ... [ENT] entity_type_m [SEP] token_1 token_2 ... token_n\n",
      "\n",
      "            # update length of the sequence (add prompt length to the original length)\n",
      "            new_length[i] = new_length[i] + len(entity_prompt)\n",
      "            # update tokens\n",
      "            new_tokens.append(tokens_p)\n",
      "            # store prompt length\n",
      "            all_len_prompt.append(len(entity_prompt))\n",
      "\n",
      "        # create a mask using num_classes_all (0, if it exceeds the number of classes, 1 otherwise)\n",
      "        max_num_classes = max(num_classes_all)\n",
      "        entity_type_mask = torch.arange(max_num_classes).unsqueeze(0).expand(len(num_classes_all), -1).to(\n",
      "            x[\"span_mask\"].device)\n",
      "        entity_type_mask = entity_type_mask < torch.tensor(num_classes_all).unsqueeze(-1).to(\n",
      "            x[\"span_mask\"].device)  # [batch_size, max_num_classes]\n",
      "\n",
      "        # compute all token representations\n",
      "        bert_output = self.token_rep_layer(new_tokens, new_length)\n",
      "        word_rep_w_prompt = bert_output[\"embeddings\"]  # embeddings for all tokens (with prompt)\n",
      "        mask_w_prompt = bert_output[\"mask\"]  # mask for all tokens (with prompt)\n",
      "\n",
      "        # get word representation (after [SEP]), mask (after [SEP]) and entity type representation (before [SEP])\n",
      "        word_rep = []  # word representation (after [SEP])\n",
      "        mask = []  # mask (after [SEP])\n",
      "        entity_type_rep = []  # entity type representation (before [SEP])\n",
      "        for i in range(len(x[\"tokens\"])):\n",
      "            prompt_entity_length = all_len_prompt[i]  # length of prompt for this example\n",
      "            # get word representation (after [SEP])\n",
      "            word_rep.append(word_rep_w_prompt[i, prompt_entity_length:prompt_entity_length + x[\"seq_length\"][i]])\n",
      "            # get mask (after [SEP])\n",
      "            mask.append(mask_w_prompt[i, prompt_entity_length:prompt_entity_length + x[\"seq_length\"][i]])\n",
      "\n",
      "            # get entity type representation (before [SEP])\n",
      "            entity_rep = word_rep_w_prompt[i, :prompt_entity_length - 1]  # remove [SEP]\n",
      "            entity_rep = entity_rep[0::2]  # it means that we take every second element starting from the second one\n",
      "            entity_type_rep.append(entity_rep)\n",
      "\n",
      "        # padding for word_rep, mask and entity_type_rep\n",
      "        word_rep = pad_sequence(word_rep, batch_first=True)  # [batch_size, seq_len, hidden_size]\n",
      "        mask = pad_sequence(mask, batch_first=True)  # [batch_size, seq_len]\n",
      "        entity_type_rep = pad_sequence(entity_type_rep, batch_first=True)  # [batch_size, len_types, hidden_size]\n",
      "\n",
      "        # compute span representation\n",
      "        word_rep = self.rnn(word_rep, mask)\n",
      "        span_rep = self.span_rep_layer(word_rep, span_idx)\n",
      "\n",
      "        # compute final entity type representation (FFN)\n",
      "        entity_type_rep = self.prompt_rep_layer(entity_type_rep)  # (batch_size, len_types, hidden_size)\n",
      "        num_classes = entity_type_rep.shape[1]  # number of entity types\n",
      "\n",
      "        # similarity score\n",
      "        scores = torch.einsum(\"BLKD,BCD->BLKC\", span_rep, entity_type_rep)\n",
      "\n",
      "        return scores, num_classes, entity_type_mask\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(model.compute_score_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "import io\n",
    "\n",
    "from torch.utils.data.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CultCode(Dataset):\n",
    "    def __init__(self, train_on_single_tags=False, ):\n",
    "        cult_code = json.load(io.open('./datasets/related/cult_code.json'))\n",
    "        markups = cult_code['dataset']['markups']\n",
    "\n",
    "        self.texts = []\n",
    "        self.uno_span = []\n",
    "        self.multi_span = []\n",
    "        for i, mrkp in enumerate(markups):\n",
    "            self.texts.append(mrkp['text'])\n",
    "            \n",
    "            for rel in mrkp['relations']:\n",
    "                if train_on_single_tags and len(rel['tags']) > 1:\n",
    "                    continue\n",
    "                \n",
    "                if len(rel['spans']) == 1:\n",
    "                    begin = mrkp['spans'][rel['spans'][0]]['begin']\n",
    "                    end   = mrkp['spans'][rel['spans'][0]]['end']\n",
    "                    self.uno_span.append([i, [begin, end]])\n",
    "                \n",
    "                elif len(rel['spans']) > 1:\n",
    "                    mult = []\n",
    "                    for span in rel['spans']:\n",
    "                        begin = mrkp['spans'][span]['begin']\n",
    "                        end   = mrkp['spans'][span]['end']\n",
    "                        mult.append([begin, end])\n",
    "                    self.multi_span.append((i, mult))\n",
    "    \n",
    "    def get_len_eval(self):\n",
    "        return len(self.multi_span)\n",
    "    \n",
    "    def get_item_eval(self, index):\n",
    "        i, span = self.multi_span[index]\n",
    "        return {\n",
    "            'text': self.texts[i],\n",
    "            'span': span\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.uno_span)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        i, span = self.uno_span[index]\n",
    "        return {\n",
    "            'text': self.texts[i],\n",
    "            'span': span\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CultCode(train_on_single_tags=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2094"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CultCode(train_on_single_tags=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "models-research-YeHw3bbB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
